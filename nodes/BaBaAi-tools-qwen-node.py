import os
import uuid
import folder_paths
import numpy as np
import re
import random
import time
from collections import Counter
import hashlib
import io

from PIL import Image
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    Qwen3VLForConditionalGeneration,
    BitsAndBytesConfig,
    AutoProcessor,
    AutoTokenizer,
    AutoModelForCausalLM,
    TextStreamer
)
from pathlib import Path
from comfy_api.input import VideoInput
import torch

class CustomTextStreamer(TextStreamer):
    def __init__(self, tokenizer, skip_prompt=True, skip_special_tokens=True):
        super().__init__(tokenizer, skip_prompt=skip_prompt, skip_special_tokens=skip_special_tokens)
        self.generated_text = ""
        self.stop_flag = False
        self.init_time = time.time()
        self.end_time = None
        self.first_token_time = None
        self.token_count = 0

    def on_finalized_text(self, text: str, stream_end: bool = False):
        if self.first_token_time is None and text.strip():
            self.first_token_time = time.time()
        self.generated_text += text
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        self.token_count += len(tokens)
        
        if stream_end:
            self.end_time = time.time()
        if self.stop_flag:
            raise StopIteration

    def stop_generation(self):
        self.stop_flag = True
        self.end_time = time.time()

    def get_metrics(self):
        if self.end_time is None:
            self.end_time = time.time()
        total_time = self.end_time - self.init_time
        tokens_per_second = self.token_count / total_time if total_time > 0 else 0
        first_token_latency = (self.first_token_time - self.init_time) if self.first_token_time is not None else None
        
        metrics = {
            "init_time": self.init_time,
            "first_token_time": self.first_token_time,
            "first_token_latency": first_token_latency,
            "end_time": self.end_time,
            "total_time": total_time,
            "total_tokens": self.token_count,
            "tokens_per_second": tokens_per_second
        }
        
        metrics_str = f"Metrics:\n"
        for key, value in metrics.items():
            metrics_str += f"  {key}: {value}\n"
        return metrics_str

def temp_video(video: VideoInput, seed):
    unique_id = uuid.uuid4().hex
    video_path = Path(folder_paths.temp_directory) / f"temp_video_{seed}_{unique_id}.mp4"
    video_path.parent.mkdir(parents=True, exist_ok=True)
    video.save_to(os.path.join(video_path), format="mp4", codec="h264")
    return video_path.as_posix()

def temp_image(image, seed):
    unique_id = uuid.uuid4().hex
    image_path = Path(folder_paths.temp_directory) / f"temp_image_{seed}_{unique_id}.png"
    image_path.parent.mkdir(parents=True, exist_ok=True)
    img = Image.fromarray(np.clip(255.0 * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))
    img.save(os.path.join(image_path))
    return image_path.as_posix()

def temp_batch_image(image, num_counts, seed):
    image_batch_path = Path(folder_paths.temp_directory) / "Multiple"
    image_batch_path.mkdir(parents=True, exist_ok=True)
    image_paths = []
    for Nth_count in range(num_counts):
        img = Image.fromarray(np.clip(255.0 * image[Nth_count].cpu().numpy().squeeze(), 0, 255).astype(np.uint8))
        unique_id = uuid.uuid4().hex
        image_path = image_batch_path / f"temp_image_{seed}_{Nth_count}_{unique_id}.png"
        img.save(os.path.join(image_path))
        image_paths.append(image_path.resolve().as_posix())
    return image_paths

model_directory = os.path.join(folder_paths.models_dir, "VLM")
os.makedirs(model_directory, exist_ok=True)

class Qwen_VL_Model_Loader_Base:
    def load_model(self, model_class, model_repo_id, quantization, attention):
        model_dict = {"model": "", "model_path": ""}
        model_name = model_repo_id.rsplit("/", 1)[-1]
        model_path = os.path.join(model_directory, model_name)
        
        if not os.path.exists(model_path):
            print(f"Downloading {model_name} to: {model_path}")
            from huggingface_hub import snapshot_download
            snapshot_download(repo_id=model_repo_id, local_dir=model_path, local_dir_use_symlinks=False)

        quant_config = None
        if quantization == "4bit":
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                llm_int8_enable_fp32_cpu_offload=True
            )
        elif quantization == "8bit":
            quant_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True
            )

        print(f"[BaBaAi-Tools] Attempting to load model with '{attention}' attention...")

        model_instance = model_class.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="balanced",
            attn_implementation=attention,
            quantization_config=quant_config,
            trust_remote_code=True,
        )

        effective_attn = getattr(model_instance.config, "_attn_implementation", "unknown")
        print(f"[BaBaAi-Tools] Model '{model_name}' loaded successfully.")
        print(f"[BaBaAi-Tools] >>> Effective Attention Implementation: {effective_attn.upper()} <<<")
        if attention == "flash_attention_2" and effective_attn != "flash_attention_2":
            print("[BaBaAi-Tools] WARNING: FlashAttention 2 was requested but is not active. The model has fallen back to a different implementation. Check your GPU, CUDA, and flash-attn installation.")

        model_dict["model"] = model_instance
        model_dict["model_path"] = model_path
        
        if "Qwen3" in model_class.__name__:
            model_dict["processor"] = AutoProcessor.from_pretrained(model_path)
            
        return (model_dict,)

class Qwen2_5_VLModelLoader(Qwen_VL_Model_Loader_Base):
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "model": (["unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit", "huihui-ai/Qwen2.5-VL-3B-Instruct-abliterated"], {"default": "huihui-ai/Qwen2.5-VL-3B-Instruct-abliterated"}),
            "quantization": (["none", "4bit", "8bit"], {"default": "4bit"}),
            "attention": (["flash_attention_2", "sdpa", "eager"], {"default": "flash_attention_2"}),
        }}
    RETURN_TYPES = ("QWEN_VL_MODEL",)
    RETURN_NAMES = ("Qwen_VL_model",)
    FUNCTION = "load_2_5"
    CATEGORY = "ğŸ“œBaBaAi Tools"
    def load_2_5(self, model, quantization, attention):
        return self.load_model(Qwen2_5_VLForConditionalGeneration, model, quantization, attention)

class Qwen3_VL_ModelLoader(Qwen_VL_Model_Loader_Base):
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "model": (["unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit", "huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated", "huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated"], {"default": "huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated"}),
            "quantization": (["none", "4bit", "8bit"], {"default": "4bit"}),
            "attention": (["flash_attention_2", "sdpa", "eager"], {"default": "flash_attention_2"}),
        }}
    RETURN_TYPES = ("QWEN_VL_MODEL",)
    RETURN_NAMES = ("Qwen_VL_model",)
    FUNCTION = "load_3"
    CATEGORY = "ğŸ“œBaBaAi Tools"
    def load_3(self, model, quantization, attention):
        return self.load_model(Qwen3VLForConditionalGeneration, model, quantization, attention)

class Qwen_VL_Run_Advanced:
    instruction_base_single = "You are a creative prompt engineer. Your mission is to analyze the provided image and generate exactly 1 distinct image transformation *instructions*.\n\nThe brief:\n\n"
    common_suffix_single = "\n\nYour response must be a single, complete, and concise instruction ready for the image editing AI. Do not add any conversational text, explanations, deviations, or numbering."
    local_edit_suffix = "Finally, be sure to add the following after the prompt: This transformation must only affect the specified regio, Do not modify facial features, body structure, background, pose, lighting, or any other part of the image, Changes must be localized and preserve global consistency."
    _PRESET_PROMPT_MAP = {
        "æ— æŒ‡ä»¤": "",
        "æ–‡æœ¬æŒ‡ä»¤-æç¤ºè¯å¢å¼ºï¼ˆNSFWï¼‰": "Generate a concise, explicit, and vivid enhancement for this text prompt, transforming it into a sharply detailed erotic scene optimized for AI image generation. Expand only on core suggested elements with sexual or intense interactions, using raw, immersive language from adult eroticaâ€”like 'throbbing cock slamming into her dripping pussy,' 'sweat-glistened tits bouncing mid-fuck,' or 'cum-splattered face in ecstasy'â€”to amp up the heat without excess. Match the vulgarity to the prompt's intensity: build sensual tension from tame hints like a director framing a porn still, avoiding unrelated extremes or gratuitous additions. Never sanitize explicit cues, but don't bloat neutral details into filth. Tune the tone to the prompt's essenceâ€”sultry tease, savage dominance, or fetish edgeâ€”and spotlight vivid visuals like arched backs in mid-thrust, torn lingerie clinging to curves, gritted teeth in orgasmic snarl, and glistening fluids on taut skin. Limit output to 150-200 words max for punchy, focused prompts; keep it evocative and precise, ditching vague fluff for hyper-specific, frame-ready eroticism.",
        "æ–‡æœ¬æŒ‡ä»¤-æç¤ºè¯å¢å¼ºï¼ˆFluxï¼‰": "You are a prompt enhancer for the Flux image generation model. Given a short Chinese phrase describing any subjectâ€”person, animal, object, or sceneâ€”translate it into English and expand it into a single, richly detailed and visually evocative prompt. By default, generate descriptions in the style of realistic photography, including details such as subject appearance, pose, clothing or texture, environment, lighting, mood, and camera perspective. If the original phrase explicitly mentions an artistic style (e.g., flat illustration, oil painting, cyberpunk), adapt the description to match that style instead. Do not provide multiple options or explanationsâ€”just output one complete, vivid paragraph suitable for high-quality image synthesis.",
        "æ–‡æœ¬æŒ‡ä»¤-æç¤ºè¯å¢å¼ºï¼ˆFluxä¸­æ–‡ï¼‰": "You are a prompt enhancement assistant for the Flux image generation model. Users will enter a short Chinese phrase, possibly describing a person, animal, object, or scene. Please expand this phrase into a complete, detailed, and visually expressive Chinese prompt. By default, please describe the subject in a realistic photographic style, including details such as the subject's appearance, pose, clothing or material, environment, lighting, atmosphere, and camera angle. If the original phrase explicitly mentions an artistic style (such as flat illustration, oil painting, cyberpunk, etc.), please construct the prompt based on that style. Do not provide multiple options or explanations, and only output a complete Chinese prompt suitable for high-quality image generation.",
        "æ–‡æœ¬æŒ‡ä»¤-æç¤ºè¯å¢å¼ºï¼ˆæ ‡ç­¾ï¼‰": "Expand the following input into tag-style phrases suitable for Stable Diffusion 1.5 or SDXL. The output must consist of short, descriptive English keywords onlyâ€”no full sentences or natural language. Follow this structure and order: Character Features (e.g., age, gender, appearance, clothing), Environment (e.g., background, setting), Lighting (e.g., time of day, light type), View Angle (e.g., close-up, wide shot, top-down), and Image Quality (e.g., high quality, 8k, cinematic). If the original input is in Chinese, translate it into English first. Only generate one set of tagsâ€”do not provide multiple options or explanations.",
        "æ–‡æœ¬æŒ‡ä»¤-æç¤ºè¯å¢å¼ºï¼ˆWanï¼‰": "You are a cinematic prompt composer for the wan2.2 video generation model. Based on the following simplified input text, expand it into a fully detailed video prompt. Your output must include descriptive elements for (1) subject appearance and identity, (2) scene and environment, (3) specific motion/action events, (4) cinematic composition and camera behavior, (5) lighting and color palette, (6) stylization and aesthetic tone. Construct a visually rich, dramatic and filmic scene suitable for high-quality video synthesis. Use natural language with vivid imagery and maintain temporal continuity. Do not over-describe static elementsâ€”favor dynamic and expressive language. Do not add any conversational text, explanations, or deviations",
        "æ–‡æœ¬æŒ‡ä»¤-æç¤ºè¯å¢å¼ºï¼ˆWanä¸­æ–‡ï¼‰": "You are a cinematic prompt composer for the wan2.2 video generation model. Based on the following simplified input text, expand it into a fully detailed video Chinese prompt. Your output must include descriptive elements for (1) subject appearance and identity, (2) scene and environment, (3) specific motion/action events, (4) cinematic composition and camera behavior, (5) lighting and color palette, (6) stylization and aesthetic tone. Construct a visually rich, dramatic and filmic scene suitable for high-quality video synthesis. Use natural language with vivid imagery and maintain temporal continuity. Do not over-describe static elementsâ€”favor dynamic and expressive language. Do not add any conversational text, explanations, or deviations, and only output a complete Chinese prompt suitable for high-quality video generation.",
        "æ–‡æœ¬æŒ‡ä»¤-æ–‡æœ¬æ‰“æ ‡": "Transform the text into tags, separated by commas, donâ€™t use duplicate tags",
        "æ–‡æœ¬æŒ‡ä»¤-ç¿»è¯‘æˆä¸­æ–‡": "Translate this text into Chinese like translation software, and the results only need to be displayed in Chinese",
        "æ–‡æœ¬æŒ‡ä»¤-ç¿»è¯‘æˆè‹±æ–‡": "Translate this text into English like translation software, and the results only need to be displayed in English",
        "æ–‡æœ¬æŒ‡ä»¤-å°è¯´ç»“æ„åŒ–": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´æ–‡æœ¬ç»“æ„åŒ–å¤„ç†å™¨ã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™å¤„ç†è¾“å…¥æ–‡æœ¬ï¼š
1. å°†æ–‡æœ¬æ‹†åˆ†ä¸º`<Narrator>`å™è¿°æ®µè½å’Œ`<CharacterX>`è§’è‰²å¯¹è¯
2. è§’è‰²åˆ†é…è§„åˆ™ï¼š
    - åŒä¸€è§’è‰²å§‹ç»ˆä½¿ç”¨ç›¸åŒCharacterç¼–å·ï¼ˆå¦‚ç‹é‡å§‹ç»ˆæ˜¯<Character1>ï¼‰
    - æ–°è§’è‰²é¦–æ¬¡å‡ºç°æ—¶è‡ªåŠ¨åˆ†é…æ–°ç¼–å·
    - è§’è‰²è¯†åˆ«ä¼˜å…ˆçº§ï¼šè§’è‰²å > ä»£è¯(ä»–/å¥¹) > ç‰¹å¾æè¿°
    - å¦‚ç‰¹å¾æ¨¡ç³Šè¯·ä»æ•´ä¸ªæ–‡æœ¬å†…å®¹å»åˆ†æå¦‚ä½•åˆ†é…ç¼–å·
3. æ–‡æœ¬åˆ†ç±»è§„åˆ™ï¼š
    - ç›´æ¥å¼•è¯­å½’å…¥è§’è‰²æ ‡ç­¾
    - åŠ¨ä½œ/ç¯å¢ƒ/å¿ƒç†æå†™å½’å…¥Narrator
    - å¯¹è¯å¼•å¯¼è¯­ï¼ˆå¦‚"ç‹é‡è¯´é“ï¼š"ï¼‰å½’å…¥Narrator
4. è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
    - æ¯æ®µç‹¬ç«‹ä¸€è¡Œï¼Œç”¨æŒ‡å®šæ ‡ç­¾åŒ…è£¹
    - ä¸¥æ ¼ä¿æŒåŸæ–‡æ ‡ç‚¹å’Œæ¢è¡Œ
    - ä¸æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜æˆ–æ³¨é‡Š
### è¾“å‡ºç¤ºä¾‹
è¾“å…¥æ–‡æœ¬ï¼š
'''
å°‘å¥³æ­¤æ—¶å°±ç«™åœ¨é™¢å¢™é‚£è¾¹ï¼Œå¥¹æœ‰ä¸€åŒæçœ¼ï¼Œæ€¯æ€¯å¼±å¼±ã€‚
é™¢é—¨é‚£è¾¹ï¼Œæœ‰ä¸ªå—“éŸ³è¯´ï¼šâ€œä½ è¿™å©¢å¥³å–ä¸å–ï¼Ÿâ€
å®‹é›†è–ªæ„£äº†æ„£ï¼Œå¾ªç€å£°éŸ³è½¬å¤´æœ›å»ï¼Œæ˜¯ä¸ªçœ‰çœ¼å«ç¬‘çš„é”¦è¡£å°‘å¹´ï¼Œç«™åœ¨é™¢å¤–ï¼Œä¸€å¼ å…¨ç„¶é™Œç”Ÿçš„é¢å­”ã€‚
é”¦è¡£å°‘å¹´èº«è¾¹ç«™ç€ä¸€ä½èº«æé«˜å¤§çš„è€è€…ï¼Œé¢å®¹ç™½çš™ï¼Œè„¸è‰²å’Œè”¼ï¼Œè½»è½»çœ¯çœ¼æ‰“é‡ç€ä¸¤åº§æ¯—é‚»é™¢è½çš„å°‘å¹´å°‘å¥³ã€‚
è€è€…çš„è§†çº¿åœ¨é™ˆå¹³å®‰ä¸€æ‰«è€Œè¿‡ï¼Œå¹¶æ— åœæ»ï¼Œä½†æ˜¯åœ¨å®‹é›†è–ªå’Œå©¢å¥³èº«ä¸Šï¼Œå¤šæœ‰åœç•™ï¼Œç¬‘æ„æ¸æ¸æµ“éƒã€‚
å®‹é›†è–ªæ–œçœ¼é“ï¼šâ€œå–ï¼æ€ä¹ˆä¸å–ï¼â€
é‚£å°‘å¹´å¾®ç¬‘é“ï¼šâ€œé‚£ä½ è¯´ä¸ªä»·ã€‚â€
å°‘å¥³çªå¤§çœ¼çœ¸ï¼Œæ»¡è„¸åŒªå¤·æ‰€æ€ï¼Œåƒä¸€å¤´æƒŠæ…Œå¤±æªçš„å¹´å¹¼éº‹é¹¿ã€‚
å®‹é›†è–ªç¿»äº†ä¸ªç™½çœ¼ï¼Œä¼¸å‡ºä¸€æ ¹æ‰‹æŒ‡ï¼Œæ™ƒäº†æ™ƒï¼Œâ€œç™½é“¶ä¸€ä¸‡ä¸¤ï¼â€
é”¦è¡£å°‘å¹´è„¸è‰²å¦‚å¸¸ï¼Œç‚¹å¤´é“ï¼šâ€œå¥½ã€‚â€
å®‹é›†è–ªè§é‚£å°‘å¹´ä¸åƒæ˜¯å¼€ç©ç¬‘çš„æ ·å­ï¼Œè¿å¿™æ”¹å£é“ï¼šâ€œæ˜¯é»„é‡‘ä¸‡ä¸¤ï¼â€
é”¦è¡£å°‘å¹´å˜´è§’ç¿˜èµ·ï¼Œé“ï¼šâ€œé€—ä½ ç©çš„ã€‚â€
å®‹é›†è–ªè„¸è‰²é˜´æ²‰ã€‚
'''
æ­£ç¡®è¾“å‡ºï¼š
<Narrator>å°‘å¥³æ­¤æ—¶å°±ç«™åœ¨é™¢å¢™é‚£è¾¹ï¼Œå¥¹æœ‰ä¸€åŒæçœ¼ï¼Œæ€¯æ€¯å¼±å¼±ã€‚</Narrator>
<Narrator>é™¢é—¨é‚£è¾¹ï¼Œæœ‰ä¸ªå—“éŸ³è¯´ï¼š</Narrator>
<Character2>â€œä½ è¿™å©¢å¥³å–ä¸å–ï¼Ÿâ€</Character2>
<Narrator>å®‹é›†è–ªæ„£äº†æ„£ï¼Œå¾ªç€å£°éŸ³è½¬å¤´æœ›å»ï¼Œæ˜¯ä¸ªçœ‰çœ¼å«ç¬‘çš„é”¦è¡£å°‘å¹´ï¼Œç«™åœ¨é™¢å¤–ï¼Œä¸€å¼ å…¨ç„¶é™Œç”Ÿçš„é¢å­”ã€‚</Narrator>
<Narrator>é”¦è¡£å°‘å¹´èº«è¾¹ç«™ç€ä¸€ä½èº«æé«˜å¤§çš„è€è€…ï¼Œé¢å®¹ç™½çš™ï¼Œè„¸è‰²å’Œè”¼ï¼Œè½»è½»çœ¯çœ¼æ‰“é‡ç€ä¸¤åº§æ¯—é‚»é™¢è½çš„å°‘å¹´å°‘å¥³ã€‚</Narrator>
<Narrator>è€è€…çš„è§†çº¿åœ¨é™ˆå¹³å®‰ä¸€æ‰«è€Œè¿‡ï¼Œå¹¶æ— åœæ»ï¼Œä½†æ˜¯åœ¨å®‹é›†è–ªå’Œå©¢å¥³èº«ä¸Šï¼Œå¤šæœ‰åœç•™ï¼Œç¬‘æ„æ¸æ¸æµ“éƒã€‚</Narrator>
<Narrator>å®‹é›†è–ªæ–œçœ¼é“ï¼š</Narrator>
<Character1>â€œå–ï¼æ€ä¹ˆä¸å–ï¼â€</Character1>
<Narrator>é‚£å°‘å¹´å¾®ç¬‘é“ï¼š</Narrator>
<Character2>â€œé‚£ä½ è¯´ä¸ªä»·ã€‚â€</Character2>
<Narrator>å°‘å¥³çªå¤§çœ¼çœ¸ï¼Œæ»¡è„¸åŒªå¤·æ‰€æ€ï¼Œåƒä¸€å¤´æƒŠæ…Œå¤±æªçš„å¹´å¹¼éº‹é¹¿ã€‚</Narrator>
<Narrator>å®‹é›†è–ªç¿»äº†ä¸ªç™½çœ¼ï¼Œä¼¸å‡ºä¸€æ ¹æ‰‹æŒ‡ï¼Œæ™ƒäº†æ™ƒï¼Œ</Narrator>
<Character1>â€œç™½é“¶ä¸€ä¸‡ä¸¤ï¼â€</Character1>
<Narrator>é”¦è¡£å°‘å¹´è„¸è‰²å¦‚å¸¸ï¼Œç‚¹å¤´é“ï¼š</Narrator>
<Character2>â€œå¥½ã€‚â€</Character2>
<Narrator>å®‹é›†è–ªè§é‚£å°‘å¹´ä¸åƒæ˜¯å¼€ç©ç¬‘çš„æ ·å­ï¼Œè¿å¿™æ”¹å£é“ï¼š</Narrator>
<Character1>â€œæ˜¯é»„é‡‘ä¸‡ä¸¤ï¼â€</Character1>
<Narrator>é”¦è¡£å°‘å¹´å˜´è§’ç¿˜èµ·ï¼Œé“ï¼š</Narrator>
<Character2>â€œé€—ä½ ç©çš„ã€‚â€</Character2>
<Narrator>å®‹é›†è–ªè„¸è‰²é˜´æ²‰ã€‚</Narrator>""",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡æè¿°": "Describe the image in detail and accurately",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡æè¿°ï¼ˆä¸­æ–‡ï¼‰": "è¯¦ç»†æå™è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚è¾“å‡ºä¸­æ–‡æç¤ºè¯ã€‚",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡æè¿°ï¼ˆNSFWï¼‰": "Generate an explicit and vivid caption for this image that accurately describes all visible elements, including sexual or violent actions. Use intense language typical of adult themes, incorporating terms like 'fucking,' 'gets fucked,' 'dick,' 'pussy,' 'cum,' or other appropriate words to the descripe. But match the intensity of your description to what is actually shown, like porn-director or film-director. Don't sanitize explicit content, but also don't make innocent content sound more vulgar than it is. Ensure the tone aligns with the image's contentâ€”whether sensual, aggressive, or fetishisticâ€”and highlight specific details such as body positions, clothing, facial expressions, and any explicit acts. Maintain clarity and avoid vague terms.",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡æ‰“æ ‡": "Use tags to briefly and accurately describe this image, separated by commas, donâ€™t use duplicate tags",
        "å›¾ç‰‡æŒ‡ä»¤-ä¸‹ä¸€ä¸ªé•œå¤´ï¼ˆæ ‡å‡†ï¼‰": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå½±å¯¼æ¼”åŠ©ç†å’Œ AI å›¾åƒç”Ÿæˆæç¤ºè¯ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸º next-scene-qwen-image-lora-2509 LoRA æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æç¤ºè¯ã€‚
ä½ çš„ç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä¸ªâ€œæ ‡å‡†çš„ä¸‹ä¸€ä¸ªé•œå¤´â€ã€‚è¿™ä¸ªé•œå¤´å¿…é¡»ä¸å‰ä¸€å¸§æœ‰æ¸…æ™°ã€æ˜ç¡®ã€åˆä¹é€»è¾‘çš„è¿›å±•ã€‚å®ƒä¸è¿½æ±‚é‚£ç§â€œæˆå‰§æ€§çªå˜â€ï¼Œä¹Ÿé¿å…é‚£ç§â€œå¾®å°è°ƒæ•´â€ã€‚å®ƒæ¨¡æ‹Ÿçš„æ˜¯ç”µå½±å‰ªè¾‘ä¸­ä¸€ä¸ªæ ‡å‡†çš„ã€æœ‰æ„ä¹‰çš„é•œå¤´æ¨è¿›ã€‚

ç”Ÿæˆè§„åˆ™ï¼š

1ï¼Œå¼ºåˆ¶å‰ç¼€ï¼š æ¯ä¸€ä¸ªæç¤ºè¯éƒ½å¿…é¡»ä»¥è‹±æ–‡çŸ­è¯­ Next Scene: å¼€å¤´ï¼ˆå†’å·åè·Ÿä¸€ä¸ªç©ºæ ¼ï¼‰ã€‚

2ï¼Œæ ¸å¿ƒå†…å®¹ï¼ˆæ ‡å‡†è¿›å±•ï¼‰ï¼š æç¤ºè¯çš„ä¸»ä½“å¿…é¡»æ¸…æ™°åœ°æè¿°ä¸€ä¸ª**â€œå•ä¸€ä¸”æ˜ç¡®â€**çš„å˜åŒ–ã€‚ä½ å¿…é¡»ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©ä¸€ä¸ªä½œä¸ºä¸»è¦é©±åŠ¨åŠ›ï¼š
    A. æ¸…æ™°çš„æ‘„åƒæœºè¿åŠ¨ (Clear Camera Movement)ï¼š æè¿°ä¸€ä¸ªå®Œæ•´çš„ã€æœ‰ç›®çš„çš„æ‘„åƒæœºè¿åŠ¨ã€‚ï¼ˆé¿å…ä½¿ç”¨â€œè½»å¾®/slightlyâ€æˆ–â€œå¿«é€Ÿ/rapidlyâ€ï¼‰
        å…³é”®è¯ç¤ºä¾‹ï¼š The camera pushes in from a medium shot to a close-up on the character's hands... (é•œå¤´ä»ä¸­æ™¯æ¨è¿›åˆ°è§’è‰²åŒæ‰‹çš„ç‰¹å†™), The camera pulls back to reveal the character is not alone... (é•œå¤´æ‹‰å›ï¼Œæ­ç¤ºè§’è‰²ä¸æ˜¯ä¸€ä¸ªäºº), The camera tracks alongside the character as they walk... (é•œå¤´è·Ÿéšè§’è‰²è¡Œèµ°), The camera pans left to follow the car driving away... (é•œå¤´å‘å·¦å¹³ç§»ï¼Œè·Ÿéšæ±½è½¦é©¶ç¦»)ã€‚
    B. æ˜ç¡®çš„å‰ªè¾‘åˆ‡æ¢ (Clear Cut / Shot Change)ï¼š æ˜ç¡®æè¿°ä¸€ä¸ªæ–°é•œå¤´çš„æ™¯åˆ«æˆ–è§’åº¦ï¼Œæš—ç¤ºè¿™æ˜¯ä¸€ä¸ªâ€œå‰ªè¾‘ç‚¹â€ã€‚
        å…³é”®è¯ç¤ºä¾‹ï¼š The shot cuts to a close-up of the ringing phone... (é•œå¤´åˆ‡åˆ°æ­£åœ¨å“é“ƒçš„ç”µè¯çš„ç‰¹å†™), The shot is now a wide-angle establishing shot of the building... (ç°åœ¨çš„é•œå¤´æ˜¯å»ºç­‘ç‰©çš„å¹¿è§’å®šåœºé•œå¤´), The scene changes to an over-the-shoulder shot from behind the protagonist... (åœºæ™¯åˆ‡æ¢åˆ°ä¸»è§’èƒŒåçš„è¿‡è‚©é•œå¤´), A low-angle shot now makes the villain look intimidating... (ä¸€ä¸ªä½è§’åº¦é•œå¤´ç°åœ¨è®©åæ´¾çœ‹èµ·æ¥å¾ˆå“äºº)ã€‚
    C. åˆä¹é€»è¾‘çš„ä¸»ä½“åŠ¨ä½œ (Logical Subject Action)ï¼š æè¿°è§’è‰²æˆ–ä¸»ä½“åˆä¹æƒ…ç†çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚è¿™ä¸ªåŠ¨ä½œæ˜¯å™äº‹æ‰€å¿…éœ€çš„ã€‚
        å…³é”®è¯ç¤ºä¾‹ï¼š The character stops reading, closes the book, and looks up... (è§’è‰²åœæ­¢é˜…è¯»ï¼Œåˆä¸Šä¹¦ï¼Œç„¶åæŠ¬å¤´), The soldier raises their binoculars to scan the horizon... (å£«å…µä¸¾èµ·æœ›è¿œé•œæ‰«è§†åœ°å¹³çº¿), The cat, having heard a noise, turns its head towards the door... (çŒ«å¬åˆ°äº†å™ªéŸ³ï¼Œè½¬å¤´æœ›å‘é—¨å£), The hero lowers their shield after blocking the attack... (è‹±é›„åœ¨æŒ¡ä½æ”»å‡»åæ”¾ä¸‹äº†ç›¾ç‰Œ)ã€‚
    D. åˆç†çš„ç¯å¢ƒ/å¤§æ°”å˜åŒ– (Logical Environmental Change)ï¼š æè¿°ä¸€ä¸ªç¬¦åˆå™äº‹é€»è¾‘çš„ç¯å¢ƒå˜åŒ–ã€‚
        å…³é”®è¯ç¤ºä¾‹ï¼š Rain begins to lightly fall, and the pavement starts to get wet... (å¼€å§‹ä¸‹å°é›¨ï¼Œäººè¡Œé“å¼€å§‹å˜æ¹¿), The sun dips below the horizon, casting long shadows... (å¤ªé˜³è½å±±ï¼ŒæŠ•ä¸‹é•¿é•¿çš„å½±å­), A second character walks into the room and stops... (ç¬¬äºŒä¸ªè§’è‰²èµ°è¿›æˆ¿é—´å¹¶åœä¸‹), The door in the background opens... (èƒŒæ™¯é‡Œçš„é—¨æ‰“å¼€äº†)ã€‚

3ï¼Œé£æ ¼æè¿°ï¼š åœ¨æç¤ºè¯çš„æœ«å°¾ï¼Œæ·»åŠ æ ‡å‡†çš„ç”µå½±æ„Ÿé£æ ¼åŒ–æè¿°ã€‚
    å…³é”®è¯ç¤ºä¾‹ï¼š Cinematic style (ç”µå½±é£æ ¼), realistic lighting (å†™å®ç…§æ˜), clear composition (æ¸…æ™°çš„æ„å›¾), natural colors (è‡ªç„¶çš„è‰²å½©), good depth of field (è‰¯å¥½çš„æ™¯æ·±)ã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼ˆå¼ºè°ƒæ ‡å‡†è¿›å±•ï¼‰ï¼š

Next Scene: The camera tracks alongside the protagonist as she walks briskly down the crowded city street. Cinematic style, natural lighting, urban atmosphere. (A ç»„åˆ)
Next Scene: The shot cuts from the wide view to a close-up of the driver's eyes in the rear-view mirror. Tense mood, cinematic, shallow depth of field. (B ç»„åˆ)
Next Scene: The character, who was listening intently, now nods slowly in agreement and begins to speak. Clear composition, soft lighting, realistic cinematic. (C ç»„åˆ)
Next Scene: The camera holds steady on the house as the porch light flickers on, illuminating the front door. Cinematic, slightly moody lighting, clear focus. (D ç»„åˆ)
Next Scene: The camera pulls back from the close-up of the map to a medium shot, showing the three adventurers studying it together. Realistic lighting, collaborative mood, cinematic. (A + C ç»„åˆ)

ä½ çš„ä»»åŠ¡ï¼š
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸Šæ‰€æœ‰è§„åˆ™ï¼Œä¸ºæˆ‘ç”Ÿæˆ1ä¸ªæ–°çš„ã€å˜åŒ–æ ‡å‡†ä¸”æ¸…æ™°çš„æç¤ºè¯ã€‚""",
        "å›¾ç‰‡æŒ‡ä»¤-ä¸‹ä¸€ä¸ªé•œå¤´ï¼ˆå¼ºçƒˆï¼‰": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå½±å¯¼æ¼”åŠ©ç†å’Œ AI å›¾åƒç”Ÿæˆæç¤ºè¯ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸º next-scene-qwen-image-lora-2509 LoRA æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æç¤ºè¯ã€‚
ä½ ç°åœ¨çš„æ ¸å¿ƒä»»åŠ¡æ˜¯æœ€å¤§åŒ–å˜åŒ–ã€‚ç”Ÿæˆçš„æç¤ºè¯å¿…é¡»é©±åŠ¨ LoRA äº§ç”Ÿä¸€ä¸ªä¸å‰ä¸€å¸§æœ‰æ˜¾è‘—è§†è§‰å·®å¼‚çš„é•œå¤´ï¼Œå¿…é¡»åƒç”µå½±ä¸­ä¸€ä¸ªå…³é”®çš„â€œæ–°é•œå¤´â€ï¼Œè€Œä¸æ˜¯åŒä¸€é•œå¤´çš„è½»å¾®å»¶ç»­ã€‚ä½ è¦ä¸¥æ ¼é¿å…å¾®å¦™çš„ã€å¾®å°çš„è°ƒæ•´ã€‚

ç”Ÿæˆè§„åˆ™ï¼š

1ï¼Œå¼ºåˆ¶å‰ç¼€ï¼š æ¯ä¸€ä¸ªæç¤ºè¯éƒ½å¿…é¡»ä»¥è‹±æ–‡çŸ­è¯­ Next Scene: å¼€å¤´ï¼ˆå†’å·åè·Ÿä¸€ä¸ªç©ºæ ¼ï¼‰ã€‚

2ï¼Œæ ¸å¿ƒå†…å®¹ï¼ˆæˆå‰§æ€§è½¬æŠ˜ï¼‰ï¼š æç¤ºè¯çš„ä¸»ä½“å¿…é¡»æè¿°ä¸€ä¸ªå·¨å¤§ä¸”æ˜æ˜¾çš„åŠ¨æ€å˜åŒ–ã€‚ä½ å¿…é¡»ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©ï¼Œå¹¶ä½¿ç”¨å¼ºçƒˆçš„ã€ä¸å®¹ç½®ç–‘çš„åŠ¨è¯ï¼š
    A. å¤§å¹…åº¦çš„æ‘„åƒæœºè¿åŠ¨ä¸æ„å›¾é‡å¡‘ (Dramatic Camera Moves & Compositional Reshaping)ï¼š
        æŒ‡ä»¤ï¼š å¿…é¡»æè¿°ä¸€ä¸ªå½»åº•æ”¹å˜æ„å›¾çš„é•œå¤´è¿åŠ¨ã€‚ä¾‹å¦‚ï¼Œä»ä¸€ä¸ªæç«¯åˆ‡æ¢åˆ°å¦ä¸€ä¸ªæç«¯ï¼ˆå¹¿è§’åˆ°ç‰¹å†™ï¼‰ï¼Œæˆ–è€…ä¸€ä¸ªæ¨ªæ‰«æ•´ä¸ªåœºæ™¯çš„å®å¤§è¿é•œã€‚
        å…³é”®è¯ç¤ºä¾‹ï¼š A rapid push-in from a wide shot to an extreme close-up on... (ä»å¹¿è§’å¿«é€Ÿæ¨è¿›åˆ°...çš„å¤§ç‰¹å†™), A sweeping crane shot that moves from ground-level up high, revealing... (ä¸€ä¸ªä»åœ°é¢æ‹‰åˆ°é«˜å¤„çš„å®å¤§æ‘‡è‡‚é•œå¤´ï¼Œæ­ç¤ºäº†...), The camera abruptly pulls back from the action to reveal a massive, unexpected threat... (é•œå¤´ä»åŠ¨ä½œä¸­çŒ›ç„¶åæ‹‰ï¼Œæ­ç¤ºäº†ä¸€ä¸ªå·¨å¤§çš„ã€æ„å¤–çš„å¨èƒ), The shot transitions from a subjective (POV) view to a detached high-angle shot. (é•œå¤´ä»ä¸»è§‚è§†è§’åˆ‡æ¢åˆ°ç–ç¦»çš„ä¿¯æ‹è§†è§’)ã€‚
    B. å†³å®šæ€§çš„ä¸»ä½“åŠ¨ä½œ (Decisive Subject Action)ï¼š
        æŒ‡ä»¤ï¼š å¿…é¡»æè¿°ä¸€ä¸ªæ”¹å˜æ¸¸æˆè§„åˆ™çš„åŠ¨ä½œã€‚è¿™ä¸ªåŠ¨ä½œå¿…é¡»åœ¨å™äº‹ä¸Šæ¨è¿›æ•…äº‹ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªå°å§¿åŠ¿ã€‚
        å…³é”®è¯ç¤ºä¾‹ï¼š The character, who was standing still, now breaks into a full sprint... (è§’è‰²ä»é™æ­¢å˜ä¸ºå…¨åŠ›å†²åˆº), The protagonist suddenly ignites their powers, casting a blinding light... (ä¸»è§’çªç„¶ç‚¹ç‡ƒäº†ä»–ä»¬çš„åŠ›é‡ï¼Œå‘å‡ºè€€çœ¼çš„å…‰èŠ’), The dragon opens its mouth and unleashes a massive stream of fire... (é¾™å¼ å¼€å˜´ï¼Œå–·å°„å‡ºå·¨å¤§çš„ç«ç„°æµ), The character slams their fist on the table, scattering objects everywhere... (è§’è‰²ä¸€æ‹³ç ¸åœ¨æ¡Œä¸Šï¼Œç‰©å“å››å¤„é£æº…)ã€‚
    C. å‰§çƒˆçš„ç¯å¢ƒ/å¤§æ°”çªå˜ (Drastic Environmental/Atmospheric Shifts)ï¼š
        æŒ‡ä»¤ï¼š å¿…é¡»æè¿°ä¸€ä¸ªå®Œå…¨æ”¹å˜åœºæ™¯æ°›å›´æˆ–ç‰©ç†ç¯å¢ƒçš„äº‹ä»¶ã€‚
        å…³é”®è¯ç¤ºä¾‹ï¼š A sudden, violent sandstorm instantly engulfs the entire landscape... (ä¸€åœºçªå¦‚å…¶æ¥çš„çŒ›çƒˆæ²™å°˜æš´ç¬é—´åæ²¡äº†æ•´ä¸ªæ™¯è§‚), The volcano in the background erupts, filling the sky with ash and lava... (èƒŒæ™¯ä¸­çš„ç«å±±çˆ†å‘ï¼Œç«å±±ç°å’Œç†”å²©å……æ»¡äº†å¤©ç©º), The clear sky instantly turns into a dark, supernatural tempest... (æ™´æœ—çš„å¤©ç©ºç¬é—´å˜æˆäº†é»‘æš—çš„ã€è¶…è‡ªç„¶çš„æš´é£é›¨), The entire scene is plunged into darkness as all lights suddenly go out, replaced by a single red emergency light. (æ‰€æœ‰ç¯å…‰çªç„¶ç†„ç­ï¼Œæ•´ä¸ªåœºæ™¯é™·å…¥é»‘æš—ï¼Œå–è€Œä»£KAIçš„æ˜¯ä¸€ç›çº¢è‰²åº”æ€¥ç¯)ã€‚

3ï¼Œå¼ºåˆ¶ç»„åˆï¼ˆæ¨èï¼‰ï¼š å°½å¯èƒ½å°†ç±»åˆ« Aã€B æˆ– C ç»„åˆä½¿ç”¨ï¼Œä»¥äº§ç”Ÿæœ€å¤§çš„è§†è§‰å†²å‡»åŠ›ã€‚ï¼ˆä¾‹å¦‚ï¼šä¸€ä¸ªæ‘„åƒæœºç§»åŠ¨ å¹¶ä¸” ä¸€ä¸ªä¸»ä½“åŠ¨ä½œï¼‰ã€‚       
        
4ï¼Œé£æ ¼æè¿°ï¼š åœ¨æç¤ºè¯çš„æœ«å°¾ï¼Œæ·»åŠ ç”µå½±æ„Ÿçš„é£æ ¼åŒ–æè¿°ï¼Œä»¥æ”¯æŒè¿™ç§æˆå‰§æ€§å˜åŒ–ã€‚
    å…³é”®è¯ç¤ºä¾‹ï¼š Dramatic cinematic lighting (æˆå‰§æ€§çš„ç”µå½±ç…§æ˜), high contrast (é«˜å¯¹æ¯”åº¦), epic scale (å²è¯—è§„æ¨¡), intense atmosphere (å¼ºçƒˆçš„æ°›å›´), dynamic motion blur (åŠ¨æ€æ¨¡ç³Š), wide-angle distortion (å¹¿è§’ç•¸å˜)ã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼ˆå¼ºè°ƒæ˜æ˜¾å˜åŒ–ï¼‰ï¼š

Next Scene: The camera rapidly pushes in from a wide shot of the battlefield to an extreme close-up on the general's face, capturing the exact moment he shouts "Charge!". Dramatic lighting, intense emotion, cinematic realism. (A + B ç»„åˆ)
Next Scene: The camera pulls back dramatically from the character's hopeful expression to reveal that the entire city behind them is now in ruins. Epic scale, desolate atmosphere, high contrast cinematic. (A + C ç»„åˆ)
Next Scene: Holding on the medium shot, the clear day suddenly turns to night as a massive alien mothership eclipses the sun, casting a terrifying shadow. Abrupt lighting change, ominous mood, cinematic sci-fi. (C ç»„åˆ)
Next Scene: The camera tracks backwards quickly as the protagonist, who was cornered, suddenly leaps over the chasm towards the viewer. Dynamic motion blur, heroic pose, wide-angle lens, cinematic action. (A + B ç»„åˆ)
Next Scene: The entire cave begins to collapse; the camera shakes violently as the character dives for cover under a large rock. Chaotic atmosphere, fast motion, dusty particles, realistic cinematic. (B + C ç»„åˆ)

ä½ çš„ä»»åŠ¡ï¼š
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸Šæ‰€æœ‰è§„åˆ™ï¼Œä¸ºæˆ‘ç”Ÿæˆ1ä¸ªæ–°çš„ã€å˜åŒ–æ˜æ˜¾çš„æç¤ºè¯ã€‚""",
        "å›¾ç‰‡æŒ‡ä»¤-ä¸‹ä¸€ä¸ªé•œå¤´ï¼ˆæ ‡å‡†V2ï¼‰": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå½±å¯¼æ¼”åŠ©ç†å’Œ AI å›¾åƒç”Ÿæˆæç¤ºè¯ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸º next-scene-qwen-image-lora-2509 LoRA æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æç¤ºè¯ã€‚
ä½ çš„ç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä¸ªâ€œæ ‡å‡†çš„ä¸‹ä¸€ä¸ªé•œå¤´â€ã€‚è¿™ä¸ªé•œå¤´å¿…é¡»ä¸å‰ä¸€å¸§æœ‰æ¸…æ™°ã€æ˜ç¡®ã€åˆä¹é€»è¾‘çš„è¿›å±•ã€‚

æ ¸å¿ƒç”Ÿæˆè§„åˆ™ï¼š

1ï¼Œå¼ºåˆ¶å‰ç¼€ï¼š æ¯ä¸€ä¸ªæç¤ºè¯éƒ½å¿…é¡»ä»¥è‹±æ–‡çŸ­è¯­ Next Scene: å¼€å¤´ï¼ˆå†’å·åè·Ÿä¸€ä¸ªç©ºæ ¼ï¼‰ã€‚

2ï¼Œæ ¸å¿ƒå†…å®¹ï¼ˆæ ‡å‡†è¿›å±•ï¼‰ï¼š æç¤ºè¯çš„ä¸»ä½“å¿…é¡»æ¸…æ™°åœ°æè¿°ä¸€ä¸ªâ€œå•ä¸€ä¸”æ˜ç¡®â€çš„å˜åŒ–ã€‚

3ï¼Œ!! æ ¸å¿ƒé€»è¾‘ï¼šä¸»ä½“-åœºæ™¯è”åŠ¨æ€§ (Subject-Scene Correlation) !!
    æ­¤ä¸ºæœ€é«˜è§„åˆ™ï¼š ä½ *ç»ä¸èƒ½*ç”Ÿæˆä¸€ä¸ªåªæœ‰ç¯å¢ƒå˜åŒ–ï¼ˆå¦‚å‡ºç°å¤§å‘ï¼‰è€Œä¸»ä½“ï¼ˆå¦‚äººï¼‰ä¿æŒå®Œå…¨ä¸å˜ï¼ˆç›¸åŒå§¿åŠ¿ã€è¡¨æƒ…ã€ä½ç½®ï¼‰çš„æç¤ºè¯ã€‚
    å› æœå…³ç³»ï¼š ä¸»ä½“çš„åŠ¨ä½œ/è¡¨æƒ…/ä½ç½®å˜åŒ–ï¼Œä¸åœºæ™¯/æ‘„åƒæœºçš„å˜åŒ–ï¼Œ**å¿…é¡»**æœ‰é€»è¾‘ä¸Šçš„å› æœå…³ç³»ã€‚
    ï¼ˆæ­£ç¡®ç¤ºä¾‹ï¼šï¼‰ `Next Scene: A large chasm opens in the desert floor, causing the character to stumble backwards in surprise.` (ç¯å¢ƒå˜åŒ– -> è§’è‰²ååº”)
    ï¼ˆæ­£ç¡®ç¤ºä¾‹ï¼šï¼‰ `Next Scene: The character presses a button, and a holographic map appears in front of them.` (è§’è‰²åŠ¨ä½œ -> ç¯å¢ƒå˜åŒ–)
    ï¼ˆ**é”™è¯¯**ç¤ºä¾‹ï¼šï¼‰ `Next Scene: A large chasm opens in the desert floor, and the character continues to stand there perfectly still.`

è¯¦ç»†å†…å®¹ç±»åˆ«ï¼ˆå¿…é¡»éµå®ˆè§„åˆ™ 3ï¼‰ï¼š

A. æ¸…æ™°çš„æ‘„åƒæœºè¿åŠ¨ï¼š æè¿°ä¸€ä¸ªæœ‰ç›®çš„çš„æ‘„åƒæœºè¿åŠ¨ï¼Œé€šå¸¸æ˜¯ä¸ºäº†*è·Ÿéš*ä¸€ä¸ªåŠ¨ä½œæˆ–*æ­ç¤º*ä¸€ä¸ªååº”ã€‚
    ç¤ºä¾‹ï¼š `Next Scene: The camera tracks alongside the character as they begin to walk...` (æ‘„åƒæœºè·Ÿéšä¸»ä½“çš„åŠ¨ä½œ)
    ç¤ºä¾‹ï¼š `Next Scene: The camera pushes in on the character's face as they notice something off-screen.` (æ‘„åƒæœºè¿åŠ¨æ•æ‰ä¸»ä½“çš„ååº”)
B. æ˜ç¡®çš„å‰ªè¾‘åˆ‡æ¢ï¼š æè¿°ä¸€ä¸ªæ–°é•œå¤´ï¼Œé€šå¸¸ç”¨äºå±•ç¤ºä¸€ä¸ª*ååº”é•œå¤´*æˆ–ä¸€ä¸ªåŠ¨ä½œçš„*ç»“æœ*ã€‚
    ç¤ºä¾‹ï¼š `Next Scene: The shot cuts to a close-up of the character's hand as it reaches for the sword.` (å‰ªè¾‘ä»¥çªå‡ºä¸»ä½“çš„åŠ¨ä½œ)
    ç¤ºä¾‹ï¼š `Next Scene: The shot cuts to a wide view, showing the character is now surrounded by wolves.` (å‰ªè¾‘ä»¥æ­ç¤ºç¯å¢ƒå˜åŒ–ï¼Œå¹¶æš—ç¤ºè§’è‰²ä¸‹ä¸€æ­¥çš„ååº”)
C. åˆä¹é€»è¾‘çš„ä¸»ä½“åŠ¨ä½œ (å› )ï¼š æè¿°è§’è‰²æˆ–ä¸»ä½“åˆä¹æƒ…ç†çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œè¿™ä¸ªåŠ¨ä½œå¯èƒ½ä¼š*å¯¼è‡´*ç¯å¢ƒå˜åŒ–ã€‚
    ç¤ºä¾‹ï¼š `Next Scene: The character raises their hand, and the rocks in front of them begin to float.` (ä¸»ä½“åŠ¨ä½œ -> ç¯å¢ƒå˜åŒ–)
    ç¤ºä¾‹ï¼š `Next Scene: The character turns their head towards the door as it begins to open.` (ä¸»ä½“åŠ¨ä½œä¸ç¯å¢ƒå˜åŒ–åŒæ­¥å‘ç”Ÿ)
D. åˆç†çš„ç¯å¢ƒå˜åŒ– (æœ)ï¼š æè¿°ä¸€ä¸ªç¯å¢ƒå˜åŒ–ï¼Œå¹¶*å¼ºåˆ¶*ä¸»ä½“å¯¹å…¶åšå‡ºååº”ã€‚
    ç¤ºä¾‹ï¼š `Next Scene: Rain begins to fall, forcing the character to pull up their hood.` (ç¯å¢ƒå˜åŒ– -> ä¸»ä½“ååº”)
    ç¤ºä¾‹ï¼š `Next Scene: The wall in front of the character crumbles, revealing a hidden passage and they step back in awe.` (ç¯å¢ƒå˜åŒ– -> ä¸»ä½“ååº”)

é£æ ¼æè¿°ï¼š åœ¨æç¤ºè¯çš„æœ«å°¾ï¼Œæ·»åŠ æ ‡å‡†çš„ç”µå½±æ„Ÿé£æ ¼åŒ–æè¿°ã€‚ï¼ˆä¾‹å¦‚ï¼š`Cinematic style`, `realistic lighting`, `clear composition`ï¼‰

ä½ çš„ä»»åŠ¡ï¼š
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸Šæ‰€æœ‰è§„åˆ™ï¼Œç‰¹åˆ«æ˜¯**è§„åˆ™ 3 (ä¸»ä½“-åœºæ™¯è”åŠ¨æ€§)**ï¼Œä¸ºæˆ‘ç”Ÿæˆ1ä¸ªæ–°çš„ã€**å˜åŒ–æ ‡å‡†ä¸”é€»è¾‘ä¸¥è°¨**çš„æç¤ºè¯ã€‚""",
        "å›¾ç‰‡æŒ‡ä»¤-ä¸‹ä¸€ä¸ªé•œå¤´ï¼ˆå¼ºçƒˆV2ï¼‰": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå½±å¯¼æ¼”åŠ©ç†å’Œ AI å›¾åƒç”Ÿæˆæç¤ºè¯ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸º next-scene-qwen-image-lora-2509 LoRA æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æç¤ºè¯ã€‚
ä½ ç°åœ¨çš„æ ¸å¿ƒä»»åŠ¡æ˜¯æœ€å¤§åŒ–å˜åŒ–ï¼Œç”Ÿæˆä¸€ä¸ªä¸å‰ä¸€å¸§æœ‰*æ˜¾è‘—*è§†è§‰å·®å¼‚çš„**å…³é”®â€œæ–°é•œå¤´â€**ã€‚

æ ¸å¿ƒç”Ÿæˆè§„åˆ™ï¼š

1ï¼Œå¼ºåˆ¶å‰ç¼€ï¼š æ¯ä¸€ä¸ªæç¤ºè¯éƒ½å¿…é¡»ä»¥è‹±æ–‡çŸ­è¯­ Next Scene: å¼€å¤´ï¼ˆå†’å·åè·Ÿä¸€ä¸ªç©ºæ ¼ï¼‰ã€‚

2ï¼Œæ ¸å¿ƒå†…å®¹ï¼ˆæˆå‰§æ€§è½¬æŠ˜ï¼‰ï¼š æç¤ºè¯çš„ä¸»ä½“å¿…é¡»æè¿°ä¸€ä¸ª**å·¨å¤§ä¸”æ˜æ˜¾çš„åŠ¨æ€å˜åŒ–**ã€‚

3ï¼Œ!! æ ¸å¿ƒé€»è¾‘ï¼šæˆå‰§æ€§å› æœ (Dramatic Cause & Effect) !!
    æ­¤ä¸ºæœ€é«˜è§„åˆ™ï¼š è¿™æ˜¯â€œä¸»ä½“-åœºæ™¯è”åŠ¨æ€§â€çš„å¼ºåŒ–ç‰ˆã€‚ä¸€ä¸ªæˆå‰§æ€§çš„ç¯å¢ƒå˜åŒ–**å¿…é¡»**ä¼´éšä¸€ä¸ªæˆå‰§æ€§çš„ä¸»ä½“ååº”ã€‚åä¹‹äº¦ç„¶ã€‚
    ç»ä¸è„±èŠ‚ï¼š ä¸¥ç¦å‡ºç°â€œä¸–ç•Œæœ«æ—¥äº†ï¼Œä¸»è§’è¿˜åœ¨åŸåœ°å‘å‘†â€çš„é€»è¾‘é”™è¯¯ã€‚å˜åŒ–å¿…é¡»æ˜¯*ç›¸äº’*çš„ã€‚
    ï¼ˆæ­£ç¡®ç¤ºä¾‹ï¼šï¼‰ `Next Scene: The volcano erupts violently, forcing the character to dive for cover as ash and fire rain down.` (ç¯å¢ƒå·¨å˜ -> ä¸»ä½“å‰§çƒˆååº”)
    ï¼ˆæ­£ç¡®ç¤ºä¾‹ï¼šï¼‰ `Next Scene: The hero slams their fist into the ground, causing the entire street to crack and shatter outwards.` (ä¸»ä½“å‰§çƒˆåŠ¨ä½œ -> ç¯å¢ƒå·¨å˜)
    ï¼ˆ**é”™è¯¯**ç¤ºä¾‹ï¼šï¼‰ `Next Scene: The volcano erupts violently, and the character just stands there looking.`

è¯¦ç»†å†…å®¹ç±»åˆ«ï¼ˆå¿…é¡»éµå®ˆè§„åˆ™ 3ï¼‰ï¼š

A. å¤§å¹…åº¦çš„æ‘„åƒæœºè¿åŠ¨ï¼š æè¿°ä¸€ä¸ªå½»åº•æ”¹å˜æ„å›¾çš„é•œå¤´è¿åŠ¨ï¼Œç”¨ä»¥*æ”¾å¤§*ä¸€ä¸ªåŠ¨ä½œæˆ–ä¸€ä¸ªæˆå‰§æ€§åæœã€‚
    ç¤ºä¾‹ï¼š `Next Scene: A rapid push-in from a wide shot to the character's terrified expression as the monster breaks through the wall.` (æ‘„åƒæœºè¿åŠ¨ + ä¸»ä½“ååº” + ç¯å¢ƒå˜åŒ–)
    ç¤ºä¾‹ï¼š `Next Scene: A sweeping crane shot pulls up and away, revealing the lone character is now surrounded by a massive army, and they ready their stance.` (æ‘„åƒæœºè¿åŠ¨ + ç¯å¢ƒæ­ç¤º + ä¸»ä½“ååº”)
B. å†³å®šæ€§çš„ä¸»ä½“åŠ¨ä½œ (å› )ï¼š æè¿°ä¸€ä¸ªæ”¹å˜å±€é¢çš„åŠ¨ä½œï¼Œè¿™ä¸ªåŠ¨ä½œ*å¯¼è‡´*äº†æˆå‰§æ€§çš„åæœã€‚
    ç¤ºä¾‹ï¼š `Next Scene: The protagonist ignites their superpowers, unleashing a blinding wave of energy that vaporizes the enemies in front of them.` (ä¸»ä½“åŠ¨ä½œ -> ç¯å¢ƒ/ä»–äººå˜åŒ–)
    ç¤ºä¾‹ï¼š `Next Scene: The character, previously cornered, suddenly leaps across the chasm in a desperate jump.` (ä¸»ä½“æˆå‰§æ€§åŠ¨ä½œ)
C. å‰§çƒˆçš„ç¯å¢ƒçªå˜ (æœ)ï¼š æè¿°ä¸€ä¸ªå®Œå…¨æ”¹å˜åœºæ™¯çš„äº‹ä»¶ï¼Œå¹¶*è¿«ä½¿*ä¸»ä½“åšå‡ºå¼ºçƒˆçš„ååº”ã€‚
    ç¤ºä¾‹ï¼š `Next Scene: A sudden, violent explosion to the left throws the character off their feet.` (ç¯å¢ƒå·¨å˜ -> ä¸»ä½“è¢«åŠ¨ååº”)
    ç¤ºä¾‹ï¼š `Next Scene: The entire floor collapses, forcing the character to grab onto a ledge at the last second.` (ç¯å¢ƒå·¨å˜ -> ä¸»ä½“ååº”)
* å¼ºåˆ¶ç»„åˆï¼š å°½å¯èƒ½å°† (B)ä¸»ä½“åŠ¨ä½œ å’Œ (C)ç¯å¢ƒå˜åŒ– ç»“åˆåœ¨ä¸€ä¸ªæç¤ºè¯ä¸­ï¼Œä»¥åˆ›é€ æœ€å¼ºçƒˆçš„â€œå› æœâ€å†²å‡»åŠ›ã€‚

é£æ ¼æè¿°ï¼š åœ¨æç¤ºè¯çš„æœ«å°¾ï¼Œæ·»åŠ æ”¯æŒæˆå‰§æ€§å˜åŒ–çš„é£æ ¼åŒ–æè¿°ã€‚ï¼ˆä¾‹å¦‚ï¼š`Dramatic cinematic lighting`, `high contrast`, `epic scale`, `dynamic motion blur`ï¼‰

ä½ çš„ä»»åŠ¡ï¼š
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸Šæ‰€æœ‰è§„åˆ™ï¼Œç‰¹åˆ«æ˜¯**è§„åˆ™ 3 (æˆå‰§æ€§å› æœ)**ï¼Œä¸ºæˆ‘ç”Ÿæˆ1ä¸ªæ–°çš„ã€**å˜åŒ–æ˜æ˜¾ä¸”é€»è¾‘ä¸¥è°¨**çš„æç¤ºè¯ã€‚""",
        "å›¾ç‰‡æŒ‡ä»¤-é¦–å°¾è¿‡åº¦ï¼ˆWanï¼‰": "You are a cinematic prompt composer for the wan2.2 video generation model. You are given a composite image containing two frames: the right side is the starting frame of the video, and the left side is the ending frame. Your task is to analyze both frames and imagine a smooth, emotionally coherent transition between them. Generate a single, detailed video prompt that describes the full temporal arcâ€”from the initial pose to the final poseâ€”by constructing plausible intermediate actions, gestures, expressions, and environmental changes. Include subject movement, camera behavior, framing, lighting evolution, and pacing. The transition must feel natural and continuous, without abrupt jumps. Do not explain your reasoningâ€”just output the final prompt.",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡æè¿°ï¼ˆWanï¼‰": "You are a cinematic prompt composer for the wan2.2 video generation model. Based on the uploaded image, analyze its visual content and extrapolate a fully detailed video prompt. Your output must incorporate accurate and creative inferences of (1) subject appearance and identity, (2) setting and environmental design, (3) motion or action, (4) cinematic composition and camera behavior, (5) lighting and mood, and (6) stylization and aesthetic tone. Frame the output as a dynamic scene suitable for temporal video generation. Use vivid imagery, expressive verbs, and avoid static descriptions. Highlight subtle emotional nuance and visual storytelling potential. Do not over-describe static elementsâ€”favor dynamic and expressive language. Do not add any conversational text, explanations, or deviations",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡é¢œè‰²": "Analyze the image and extract the plain_english_colors of the 5 main colors, separated by commas",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡HEX": "Analyze the image and extract the HEX values of the 5 main colors, separated by commas",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡RGB": "Analyze the image and extract the RGB values of the 5 main colors, separated by commas and parentheses",
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-ä¼ é€": instruction_base_single + "Teleport the subject to a random location, scenario and/or style. Re-contextualize it in various scenarios that are completely unexpected. Do not instruct to replace or transform the subject, only the context/scenario/style/clothes/accessories/background..etc." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-ç§»åŠ¨ç›¸æœº": instruction_base_single + "Move the camera to reveal new aspects of the scene. Provide highly different types of camera mouvements based on the scene (eg: the camera now gives a top view of the room; side portrait view of the person..etc )." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-é‡æ‰“å…‰": instruction_base_single + "Suggest new lighting settings for the image. Propose various lighting stage and settings, with a focus on professional studio lighting.\n\nSome suggestions should contain dramatic color changes, alternate time of the day, remove or include some new natural lightsâ€¦etc" + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-é‡æ„å›¾": instruction_base_single + "Recompose the entire image while keeping the core subjects. Suggest a new framing, object placement, and visual hierarchy to improve aesthetic flow." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-äº§å“æ‘„å½±": instruction_base_single + "Turn this image into the style of a professional product photo. Describe a variety of scenes (simple packshot or the item being used), so that it could show different aspects of the item in a highly professional catalog.\n\nSuggest a variety of scenes, light settings and camera angles/framings, zoom levels, etc.\n\nSuggest at least 1 scenario of how the item is used." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-äº§å“å¤–è§‚": instruction_base_single + "You are a visual product designer. Modify the outer appearance of the product in the image while retaining its core shape and purpose. Suggest a new stylistic directionâ€”such as luxury packaging, minimalist tech casing, eco-friendly wrapping, retro appliance housing, or futuristic industrial look. Update color palette, materials, surface textures, and visible branding elements to match the new design intent." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-ç¼©æ”¾": instruction_base_single + "Analyze the image and zoom on its main subject. Provide different levels of zoom. If an explicit subject is provided within the image, focus on that; otherwise, determine the most prominent subject. Always provide diverse zoom levels.\n\nExample: Zoom on the abstract painting above the fireplace to focus on its details, capturing the texture and color variations, while slightly blurring the surrounding room for a moderate zoom effect." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-ä¸Šè‰²": instruction_base_single + "Colorize the image. Provide different color styles / restoration guidance." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-LOGOè®¾è®¡": instruction_base_single + "You are a visual identity designer. Transform the main object or character in the image into a stylized logo representation. Reduce complexity while preserving recognizable features, using abstracted forms, simplified shapes, and strong contrast. Apply flat color palette or vector aesthetics typical of professional logos, such as monochrome, minimal line work, or bold emblem style. The result should be scalable and symbolically representative, suitable for brand use." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å°¾å¸§å›¾åƒ": instruction_base_single + "You are a cinematic sequence planner. Analyze the original image and generate a final frame that complements the initial frame for high-quality video interpolation. Examine subject pose, environment, motion cues, and narrative context to infer what a compelling ending frame would look likeâ€”whether it's a resolved action, a visual climax, a spatial transition, or an emotional evolution. Ensure the tail frame introduces enough visual transformation while maintaining semantic coherence to guide the video model effectively." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-ç”µå½±æµ·æŠ¥": instruction_base_single + "Create a movie poster with the subjects of this image as the main characters. Take a random genre (action, comedy, horror, etc) and make it look like a movie poster.\n\nGenerate a stylized movie title based on the image content or infer one if clearly intended, and add relevant taglines. Also, include various textual elements typically seen in movie posters like quotes or credits." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å¡é€šåŒ–": instruction_base_single + "Turn this image into the style of a cartoon or manga or drawing. Include a reference of style, culture or time (eg: mangas from the 90s, thick lined, 3D pixar, etc)" + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-ç§»é™¤æ–‡å­—": instruction_base_single + "Remove all text from the image." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å‘å‹æ›´æ¢": instruction_base_single + "Change only the haircut of the subject. Suggest a new hairstyle with details such as cut, texture, length, and color, but preserve the rest of the image strictly unchanged. Do not modify face structure, facial expression, clothing, pose, body proportions, or the background. Ensure the new hairstyle integrates naturally on the original head position, respecting lighting and silhouette alignment. This is a localized transformation." + common_suffix_single + local_edit_suffix,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-æœè£…æ›´æ¢": instruction_base_single + "Change the subjectâ€™s clothing into a different style (e.g., traditional Hanfu, cyberpunk gear, royal garments). Maintain natural integration with pose and lighting." + common_suffix_single + local_edit_suffix,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å†™çœŸè¾¾äºº": instruction_base_single + "You are a portrait pose director. Based on the subject's inherent aura and the ambient context in the image, construct a soft idol-style composition that enhances visual rhythm and emotional subtlety. Freely select an imaginative character identity and evocative scene atmosphere that suits the sourceâ€”whether cinematic, retro, serene, stylish, ethereal, or abstract. Ensure the final pose radiates intimacy, elegance, or soft allure through finely tuned body language. Every limb angle, hand gesture, gaze direction, and emotional suggestion must be precise and coherent. Maintain strict fidelity to facial structure using facial ID locking; identity integrity must be preserved throughout." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å¥èº«è¾¾äºº": instruction_base_single + "Ask to largely increase the muscles of the subjects while keeping the same pose and context.\n\nDescribe visually how to edit the subjects so that they turn into bodybuilders and have these exagerated large muscles: biceps, abdominals, triceps, etc.\n\nYou may change the clothse to make sure they reveal the overmuscled, exagerated body." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-ç§»é™¤å®¶å…·": instruction_base_single + "Remove all furniture and all appliances from the image. Explicitely mention to remove lights, carpets, curtains, etc if present." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å®¤å†…è®¾è®¡": instruction_base_single + "You are an interior designer. Redo the interior design of this image. Imagine some design elements and light settings that could match this room and offer diverse artistic directions, while ensuring that the room structure (windows, doors, walls, etc) remains identical." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å»ºç­‘å¤–è§‚": instruction_base_single + "You are a visual architect. Modify the external appearance of the building in the image while maintaining its core structure (walls, roof, entrances, windows). Suggest a new architectural style or facade design (e.g., modern minimalist, classical European, traditional East Asian, cyberpunk skyscraper). Adjust texture, materials, colors, and ornamental details accordingly to ensure natural integration with surroundings." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-æ¸…ç†æ‚ç‰©": instruction_base_single + "You are a cleanup editor. Your mission is to detect and remove all visual clutter or unnecessary objects from the image. These cluttered elements may include random items, debris, scattered objects, cables, packaging, trash bins, storage piles, or background distractions. Ensure removal enhances the aesthetic clarity and does not affect core subjects or intentional props." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-è‰ºæœ¯é£æ ¼": instruction_base_single + "Repaint the entire image in the unmistakable style of a famous art movement. Be descriptive and evocative. For example: 'Transform the image into a Post-Impressionist painting in the style of Van Gogh, using thick, swirling brushstrokes (impasto), vibrant, emotional colors, and a dynamic sense of energy and movement in every element.'" + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-æè´¨è½¬æ¢": instruction_base_single + "Transform the material of a specific object or surface (e.g., wood to glass, cloth to metal). Indicate visual cues for texture, reflectivity, and interaction with light." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-æƒ…ç»ªå˜åŒ–": instruction_base_single + "Alter the emotional tone of the subject in the image. Suggest a clear transformation of the facial expression, body language, and atmosphere to convey a new emotion (e.g., turn neutral to joyous, anxious to serene). Maintain realism and natural transition." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å¹´é¾„å˜åŒ–": instruction_base_single + "Visibly and realistically age or de-age the main subject, representing a different life stage. Describe detailed facial and hair changes while maintaining core features." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-å­£èŠ‚å˜åŒ–": instruction_base_single + "Transform the sceneâ€™s season (e.g., from spring to winter or summer to autumn). Modify foliage, lighting, clothing, and ambient atmosphere to reflect the new season." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-åˆæˆèåˆ": instruction_base_single + "Detect any compositing mismatches between the foreground subject and background, such as inconsistent lighting, shadows, color grading, or depth cues. Perform automatic fusion to harmonize both elements by adjusting lighting direction, shadow placement, color tones, perspective alignment, and edge blending to ensure seamless realism." + common_suffix_single,
        "å›¾ç‰‡æŒ‡ä»¤-å›¾ç‰‡ç¼–è¾‘-è°ƒè‰²æ¿": """æ ¹æ®ä¸‹é¢çš„å‡ ç§é¢œè‰²æ¥å¯¹è¿™å¼ å›¾åƒçš„è‰²å½©è¿›è¡Œé‡æ–°è®¾è®¡å›¾åƒç¼–è¾‘æç¤ºè¯ï¼Œæ¯”å¦‚æŠŠå›¾åƒä¸­çš„å•ä¸ªæˆ–è€…å¤šä¸ªç‰©ä½“æ¢æˆæŸä¸ªé¢œè‰²ï¼Œç¡®ä¿æ¯ä¸€ç§é¢œè‰²éƒ½è¢«åº”ç”¨åˆ°å›¾åƒä¸Šï¼Œéœ€è¦è€ƒè™‘å›¾åƒä¸­çš„æ‰€æœ‰å› ç´ ï¼Œä¸èƒ½ä»…é’ˆå¯¹ä¸»ä½“è¿›è¡Œé¢œè‰²æ›¿æ¢ã€‚è¾“å‡ºèŒƒæœ¬ä¸ºï¼š
å°†â€œæŸç‰©ä½“â€çš„é¢œè‰²æ”¹ä¸ºâ€œæŸé¢œè‰²â€ã€‚

æ³¨æ„ä¸Šé¢åªæ˜¯ç»™ä½ å‚è€ƒçš„è¾“å‡ºæ ¼å¼æ ·æœ¬ï¼Œä½ éœ€è¦åˆ†ææˆ‘æä¾›çš„å›¾åƒæ¥é‡æ–°è®¾è®¡ï¼Œé¢œè‰²æ›´æ¢æç¤ºè¯é™åˆ¶åœ¨5è‡³10è¡Œå†…ï¼ŒåŒæ—¶éœ€è¦æ³¨æ„é¿å…å¯¹åŒä¸€ç‰©ä½“è¿›è¡Œå¤šæ¬¡é¢œè‰²ä¿®æ”¹ï¼Œå¼ºè°ƒâ€œæŠŠæŸç‰©ä½“çš„é¢œè‰²æ¢æˆæŸç§é¢œè‰²â€ï¼Œè€Œä¸æ˜¯â€œæŠŠæŸç‰©ä½“æ¢æˆæŸç§é¢œè‰²â€ã€‚æœ€åä½ éœ€è¦ä½¿ç”¨çš„é¢œè‰²å¦‚ä¸‹ï¼š""",
        "è§†é¢‘æŒ‡ä»¤-è§†é¢‘æè¿°": "Describe the video in detail and accurately",
        "è§†é¢‘æŒ‡ä»¤-é»˜ç‰‡è¡¥éŸ³": "You are a voice synthesis prompt composer for the MMAudio model. Your task is to generate realistic and expressive voice-based audio for a silent video. Analyze the visual content frame by frame, and for each segment, determine the appropriate soundâ€”whether it's speech, ambient noise, mechanical sounds, emotional vocalizations, or symbolic utterances. Your output must reflect the timing, pacing, and emotional tone of the video, matching visual actions and transitions precisely. Avoid vague or generic words; instead, write full, meaningful voice or sound content that would naturally accompany the visuals. Do not add any conversational text, explanations, deviations, or numbering.",
        "éŸ³é¢‘æŒ‡ä»¤-éŸ³é¢‘æè¿°": "Describe the audio in detail and accurately"
    }

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "optional": {"image": ("IMAGE",), "video": ("VIDEO",)},
            "required": {
                "Qwen_VL_model": ("QWEN_VL_MODEL",),
                "system_text": ("STRING", {"default": "Text output is limited to 200 words", "multiline": True}),
                "preset_prompt": (list(cls._PRESET_PROMPT_MAP.keys()), {"default": "æ— æŒ‡ä»¤"}),
                "text": ("STRING", {"default": "", "multiline": True}),
                "video_decode_method": (["torchvision", "decord", "torchcodec"], {"default": "torchvision"}),
                "max_new_tokens": ("INT", {"default": 1024, "min": 1, "max": 2048}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 1.0, "step": 0.1}),
                "top_k": ("INT", {"default": 20, "min": 0, "max": 100}),
                "min_pixels": ("INT", {"default": 256, "min": 64, "max": 1280}),
                "max_pixels": ("INT", {"default": 1280, "min": 64, "max": 2048}),
                "total_pixels": ("INT", {"default": 20480, "min": 1, "max": 24576}),
                "seed": ("INT", {"default": 1, "min": 1, "max": 0xFFFFFFFFFFFFFFFF}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "run"
    CATEGORY = "ğŸ“œBaBaAi Tools"

    def run(self, Qwen_VL_model, system_text, preset_prompt, text, video_decode_method, max_new_tokens, temperature, top_p, top_k, min_pixels, max_pixels, total_pixels, seed, image=None, video=None):
        from qwen_vl_utils import process_vision_info

        model_dict = Qwen_VL_model
        model = model_dict["model"]
        processor = model_dict.get("processor") or AutoProcessor.from_pretrained(model_dict["model_path"])

        min_pixels *= 28 * 28
        max_pixels *= 28 * 28
        total_pixels *= 28 * 28
        
        full_text = f"{self._PRESET_PROMPT_MAP.get(preset_prompt, '')}\n{text}" if preset_prompt != "æ— æŒ‡ä»¤" else text

        content = []
        if image is not None:
            num_counts = image.shape[0]
            if num_counts == 1:
                uri = temp_image(image, seed)
                content.append({"type": "image", "image": uri, "min_pixels": min_pixels, "max_pixels": max_pixels})
            else:
                image_paths = temp_batch_image(image, num_counts, seed)
                for path in image_paths:
                    content.append({"type": "image", "image": path, "min_pixels": min_pixels, "max_pixels": max_pixels})

        if video is not None:
            uri = temp_video(video, seed)
            content.append({"type": "video", "video": uri, "min_pixels": min_pixels, "max_pixels": max_pixels, "total_pixels": total_pixels})

        if full_text.strip():
            content.append({"type": "text", "text": full_text})

        messages = [
            {"role": "system", "content": system_text},
            {"role": "user", "content": content},
        ]
        
        modeltext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        os.environ["FORCE_QWENVL_VIDEO_READER"] = video_decode_method
        image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

        if video is not None: 
            default_fps = 30.0
            final_fps = default_fps

            if "videos_kwargs" in video_kwargs and isinstance(video_kwargs.get("videos_kwargs"), dict):
                if "fps" in video_kwargs["videos_kwargs"]:
                    fps_value = video_kwargs["videos_kwargs"]["fps"]
                    
                    print(f"[BaBaAi-Tools DEBUG] Original fps_value type: {type(fps_value)}, value: {repr(fps_value)}") 

                    try:
                        if isinstance(fps_value, (float, int)):
                            final_fps = float(fps_value)
                        elif isinstance(fps_value, (list, tuple)):
                            temp_fps = None
                            for item in fps_value:
                                if isinstance(item, (float, int)):
                                    temp_fps = float(item)
                                    break
                                elif isinstance(item, (list, tuple)): 
                                    for sub_item in item:
                                         if isinstance(sub_item, (float, int)):
                                             temp_fps = float(sub_item)
                                             break
                                if temp_fps is not None:
                                    break
                            
                            if temp_fps is not None:
                                final_fps = temp_fps
                            else:
                                print(f"[BaBaAi-Tools WARNING] fps_value is a sequence but contains no number or is empty: {repr(fps_value)}. Using default FPS.")
                        else:
                            print(f"[BaBaAi-Tools DEBUG] Attempting direct float conversion for non-numeric/non-sequence type.")
                            final_fps = float(fps_value)
                            
                    except Exception as e:
                        print(f"[BaBaAi-Tools ERROR] Could not extract valid FPS from value: {repr(fps_value)}. Error: {e}. Using default FPS.")
                        final_fps = default_fps 

                    if final_fps <= 0:
                       print(f"[BaBaAi-Tools WARNING] Extracted FPS ({final_fps}) is not positive. Using default FPS.")
                       final_fps = default_fps

                    video_kwargs["videos_kwargs"]["fps"] = final_fps
                    print(f"[BaBaAi-Tools DEBUG] Final fps value set in video_kwargs: {final_fps}") 
                else:
                     print("[BaBaAi-Tools WARNING] 'fps' key not found in videos_kwargs. Using default FPS.")
                     if "videos_kwargs" not in video_kwargs: video_kwargs["videos_kwargs"] = {}
                     video_kwargs["videos_kwargs"]["fps"] = default_fps 
            else:
                 print("[BaBaAi-Tools DEBUG] 'videos_kwargs' not found or not a dict in video_kwargs. Attempting to set default FPS.")
                 if "videos_kwargs" not in video_kwargs:
                     video_kwargs["videos_kwargs"] = {}
                 elif not isinstance(video_kwargs.get("videos_kwargs"), dict):
                      video_kwargs["videos_kwargs"] = {} 
                 video_kwargs["videos_kwargs"]["fps"] = default_fps

            if 'fps' in video_kwargs:
                print(f"[BaBaAi-Tools DEBUG] Removing redundant top-level 'fps' key from video_kwargs.")
                del video_kwargs['fps']

        inputs = processor(text=[modeltext], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt", **video_kwargs)
        inputs = inputs.to(model.device)
        
        do_sample = temperature > 0.0
        if seed == 0:
             seed = random.randint(0, 0xFFFFFFFFFFFFFFFF)
        torch.manual_seed(seed)

        gen_kwargs = {"max_new_tokens": max_new_tokens, "do_sample": do_sample}
        
        if do_sample:
            gen_kwargs.update({"temperature": temperature, "top_p": top_p, "top_k": top_k})

        generated_ids = model.generate(**inputs, **gen_kwargs)

        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
        output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        
        text_out = str(output_text[0])
        if "</think>" in text_out:
            text_out = text_out.split("</think>")[-1]
        text_out = re.sub(r"^[\s\u200b\xa0]+", "", text_out)

        return (text_out,)

class Qwen3_Text_Run_Advanced:
    @classmethod
    def INPUT_TYPES(s):
        all_preset_keys = list(Qwen_VL_Run_Advanced._PRESET_PROMPT_MAP.keys())
        text_only_presets = ["æ— æŒ‡ä»¤"] + [key for key in all_preset_keys if key.startswith("æ–‡æœ¬æŒ‡ä»¤-")]
        return {"required": {
            "Qwen3_model": ("QWEN3_MODEL",),
            "system_text": ("STRING", {"default": "Text output is limited to 200 words", "multiline": True}),
            "preset_prompt": (text_only_presets, {"default": "æ— æŒ‡ä»¤"}),
            "text": ("STRING", {"default": "", "multiline": True}),
            "max_new_tokens": ("INT", {"default": 16384, "min": 1, "max": 16384}),
            "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.1}),
            "top_p": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 1.0, "step": 0.1}),
            "top_k": ("INT", {"default": 20, "min": 0, "max": 100}),
            "seed": ("INT", {"default": 0, "min": 0, "max": 0xFFFFFFFFFFFFFFFF}),
        }, "optional": {"bypass_model": ("BOOLEAN", {"default": False})}}
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("text", "metrics")
    FUNCTION = "run_model"
    CATEGORY = "ğŸ“œBaBaAi Tools"

    def run_model(self, Qwen3_model, system_text, preset_prompt, text, max_new_tokens, temperature, top_p, top_k, seed, bypass_model=False):
        if bypass_model:
            return (text, "")
        
        full_text = f"{Qwen_VL_Run_Advanced._PRESET_PROMPT_MAP.get(preset_prompt, '')}\n{text}" if preset_prompt != "æ— æŒ‡ä»¤" else text
        tokenizer = Qwen3_model["tokenizer"]
        model = Qwen3_model["model"]
        messages = [
            {"role": "system", "content": system_text},
            {"role": "user", "content": full_text}
        ]
        model_inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([model_inputs], return_tensors="pt").to(model.device)
        
        do_sample = temperature > 0.0
        if seed == 0:
            seed = random.randint(0, 0xFFFFFFFFFFFFFFFF)
        torch.manual_seed(seed)
        
        streamer = CustomTextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        generate_kwargs = {"do_sample": do_sample, "max_new_tokens": max_new_tokens, "repetition_penalty": 1.2, "no_repeat_ngram_size": 2, "streamer": streamer}
        if do_sample:
            generate_kwargs.update({"temperature": temperature, "top_k": top_k, "top_p": top_p})
            
        try:
            model.generate(**model_inputs, **generate_kwargs)
            output_text = streamer.generated_text
            metrics_text = streamer.get_metrics()
            output_text = re.sub(r'<\|im_end\|>', '', output_text)
        except Exception as e:
            output_text = f"Error during generation: {e}"
            metrics_text = ""
            
        return (output_text, metrics_text)

class Qwen3ModelLoader:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "model_name": (["unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit", "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated"], {"default": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated"}),
            "quantization": (["none", "4bit", "8bit"], {"default": "4bit"}),
            "attention": (["flash_attention_2", "sdpa", "eager"], {"default": "flash_attention_2"}),
        }}
    RETURN_TYPES = ("QWEN3_MODEL",)
    RETURN_NAMES = ("Qwen3_model",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸ“œBaBaAi Tools"

    def load_model(self, model_name, quantization, attention):
        model_id = model_name.rsplit("/", 1)[-1]
        model_path = os.path.join(model_directory, model_id)
        if not os.path.exists(model_path):
            from huggingface_hub import snapshot_download
            snapshot_download(repo_id=model_name, local_dir=model_path, local_dir_use_symlinks=False)
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if quantization == "4bit":
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True, 
                bnb_4bit_compute_dtype=torch.bfloat16, 
                bnb_4bit_use_double_quant=True, 
                llm_int8_enable_fp32_cpu_offload=True,
                bnb_4bit_quant_type="nf4"
            )
            model = AutoModelForCausalLM.from_pretrained(model_path, device_map="balanced", trust_remote_code=True, quantization_config=quant_config, torch_dtype=torch.bfloat16, attn_implementation=attention)
        elif quantization == "8bit":
            quant_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True
            )
            model = AutoModelForCausalLM.from_pretrained(model_path, device_map="balanced", trust_remote_code=True, quantization_config=quant_config, attn_implementation=attention)
        else:
            model = AutoModelForCausalLM.from_pretrained(model_path, device_map="balanced", trust_remote_code=True, torch_dtype=torch.bfloat16, attn_implementation=attention)
        
        effective_attn = getattr(model.config, "_attn_implementation", "unknown")
        print(f"[BaBaAi-Tools] Model '{model_name}' loaded successfully.")
        print(f"[BaBaAi-Tools] >>> Effective Attention Implementation: {effective_attn.upper()} <<<")
        if attention == "flash_attention_2" and effective_attn != "flash_attention_2":
            print("[BaBaAi-Tools] WARNING: FlashAttention 2 was requested but is not active. The model has fallen back to a different implementation. Check your GPU, CUDA, and flash-attn installation.")
            
        return ({"model": model, "tokenizer": tokenizer},)

NODE_CLASS_MAPPINGS = {
    "Qwen2_5_VLModelLoader": Qwen2_5_VLModelLoader,
    "Qwen3_VL_ModelLoader": Qwen3_VL_ModelLoader,
    "Qwen_VL_Run_Advanced": Qwen_VL_Run_Advanced,
    "Qwen3ModelLoader": Qwen3ModelLoader,
    "Qwen3_Text_Run_Advanced": Qwen3_Text_Run_Advanced,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen2_5_VLModelLoader": "Qwen2.5-VLæ¨¡å‹åŠ è½½å™¨",
    "Qwen3_VL_ModelLoader": "Qwen3-VLæ¨¡å‹åŠ è½½å™¨",
    "Qwen_VL_Run_Advanced": "Qwen-VLé«˜çº§è¿è¡Œå™¨",
    "Qwen3ModelLoader": "Qwen3æ¨¡å‹åŠ è½½å™¨",
    "Qwen3_Text_Run_Advanced": "Qwen3æ–‡æœ¬é«˜çº§è¿è¡Œå™¨",
}